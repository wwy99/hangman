{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "kX0-sNOzNDpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. dataset prepare and env setup\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "class HangmanEnvironment:\n",
        "    def __init__(self, words: List[str], max_incorrect_guesses: int = 6, validation_split: float = 0.2):\n",
        "        self.all_words = words\n",
        "        self.max_incorrect_guesses = max_incorrect_guesses\n",
        "        self.validation_split = validation_split\n",
        "        self.train_words, self.val_words = self._split_words()\n",
        "        self.reset()\n",
        "\n",
        "    def _split_words(self) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Split the words into training and validation sets.\"\"\"\n",
        "        random.shuffle(self.all_words)\n",
        "        split_idx = int(len(self.all_words) * (1 - self.validation_split))\n",
        "        return self.all_words[:split_idx], self.all_words[split_idx:]\n",
        "\n",
        "    def reset(self, use_validation=False, new_game=True, target_word=None):\n",
        "        if new_game:\n",
        "            self.words = self.val_words if use_validation else self.train_words\n",
        "            self.target_word = target_word if target_word else random.choice(self.words)\n",
        "        self.masked_word = ['_'] * len(self.target_word)\n",
        "        self.guessed_letters = set()\n",
        "        self.incorrect_guesses = 0\n",
        "        self.done = False\n",
        "        return self.get_state()\n",
        "\n",
        "    def guess(self, letter: str):\n",
        "        if self.done or letter in self.guessed_letters:\n",
        "            return self.get_state(), 0, self.done  # Ensure 'done' is part of the returned state\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "        reward = -1\n",
        "\n",
        "\n",
        "\n",
        "        if len(self.target_word) != len(self.masked_word):\n",
        "            raise ValueError(\"Target word and masked word lengths do not match.\")\n",
        "\n",
        "        if letter in self.target_word:\n",
        "            reward = self.reveal_letters(letter)\n",
        "        else:\n",
        "            reward = -100\n",
        "            self.incorrect_guesses += 1\n",
        "\n",
        "        if self.incorrect_guesses >= self.max_incorrect_guesses or '_' not in self.masked_word:\n",
        "            self.done = True\n",
        "            reward = 1000 if '_' not in self.masked_word else -100\n",
        "\n",
        "\n",
        "\n",
        "        return self.get_state(), reward, self.done\n",
        "\n",
        "    def reveal_letters(self, letter: str) -> int:\n",
        "        \"\"\"Reveal the guessed letter in the masked word and return the reward.\"\"\"\n",
        "        reward = 0\n",
        "        for i, l in enumerate(self.target_word):\n",
        "\n",
        "            if l == letter and self.masked_word[i] == '_':\n",
        "                self.masked_word[i] = l\n",
        "                reward += 100\n",
        "        return reward\n",
        "\n",
        "    def get_state(self) -> dict:\n",
        "        \"\"\"Return the current state of the game.\"\"\"\n",
        "        return {\n",
        "            'masked_word': ' '.join(self.masked_word),\n",
        "            'incorrect_guesses': self.incorrect_guesses,\n",
        "            'guessed_letters': self.guessed_letters,\n",
        "            'done': self.done  # Ensure 'done' is included in the state\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "QF0XAfDcqSjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "class VectorizedHangmanEnvironment:\n",
        "    def __init__(self, base_env, n_envs):\n",
        "        self.envs = [deepcopy(base_env) for _ in range(n_envs)]\n",
        "        self.n_envs = n_envs\n",
        "\n",
        "    def reset(self):\n",
        "        return [env.reset() for env in self.envs]\n",
        "\n",
        "    def step(self, actions):\n",
        "        results = [self.envs[i].guess(actions[i]) for i in range(self.n_envs)]\n",
        "        next_states, rewards, dones = zip(*results)\n",
        "        return list(next_states), list(rewards), list(dones)\n"
      ],
      "metadata": {
        "id": "QSl_CRI42Qcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=10, guessed_size=26, hidden_dim=128, num_layers=4, output_size=26):\n",
        "        super(HangmanModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer for 26 letters + underscore ('_')\n",
        "        self.char_embeddings = nn.Embedding(27, embedding_dim)\n",
        "\n",
        "        self.combine_fc = nn.Linear(embedding_dim + guessed_size + 1, embedding_dim)\n",
        "\n",
        "        # GRU layers\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers,\n",
        "                          batch_first=True, bidirectional=True, dropout=0.5)\n",
        "\n",
        "        # The output layer takes input from the last bidirectional layers * number of directions (2)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        combined = F.relu(self.combine_fc(x))\n",
        "\n",
        "        gru_out, _ = self.gru(combined)\n",
        "        gru_out = self.dropout(gru_out[:, -1, :])\n",
        "\n",
        "        out = self.fc(gru_out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yYElM0m2YMe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "class HangmanModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=10):\n",
        "        super(HangmanModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2  # Increased from 1 to 2 for more depth\n",
        "\n",
        "        # Embedding layer for 26 letters + underscore ('_')\n",
        "        self.char_embeddings = nn.Embedding(27, embedding_dim)\n",
        "\n",
        "        # Increase complexity by adding more layers to the GRU\n",
        "        self.gru = nn.GRU(embedding_dim, self.hidden_dim, self.num_layers,\n",
        "                          batch_first=True, bidirectional=True, dropout=0.5)  # Increased dropout for regularization\n",
        "\n",
        "        # Linear layer takes input from both directions of GRU\n",
        "        self.fc = nn.Linear(self.hidden_dim * 2, 26)  # *2 for bidirectional output\n",
        "\n",
        "        # Increased dropout for additional regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Pass embeddings through GRU\n",
        "        gru_out, _ = self.gru(x)\n",
        "\n",
        "        # Apply dropout to the output features\n",
        "        gru_out = self.dropout(gru_out[:, -1, :])\n",
        "\n",
        "        # Pass the output through the fully connected layer\n",
        "        out = self.fc(gru_out)\n",
        "        return out\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "EG8PpJpH7T0r",
        "outputId": "432a6050-5a55-4390-d840-77ffa033ef81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass HangmanModel(nn.Module):\\n    def __init__(self, embedding_dim=10):\\n        super(HangmanModel, self).__init__()\\n        self.embedding_dim = embedding_dim\\n        self.hidden_dim = 128\\n        self.num_layers = 2  # Increased from 1 to 2 for more depth\\n\\n        # Embedding layer for 26 letters + underscore ('_')\\n        self.char_embeddings = nn.Embedding(27, embedding_dim)\\n\\n        # Increase complexity by adding more layers to the GRU\\n        self.gru = nn.GRU(embedding_dim, self.hidden_dim, self.num_layers,\\n                          batch_first=True, bidirectional=True, dropout=0.5)  # Increased dropout for regularization\\n\\n        # Linear layer takes input from both directions of GRU\\n        self.fc = nn.Linear(self.hidden_dim * 2, 26)  # *2 for bidirectional output\\n\\n        # Increased dropout for additional regularization\\n        self.dropout = nn.Dropout(0.5)\\n\\n    def forward(self, x):\\n\\n        # Pass embeddings through GRU\\n        gru_out, _ = self.gru(x)\\n\\n        # Apply dropout to the output features\\n        gru_out = self.dropout(gru_out[:, -1, :])\\n\\n        # Pass the output through the fully connected layer\\n        out = self.fc(gru_out)\\n        return out\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "whFVWCULNoqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_state(masked_word, guessed_letters, remaining_incorrect_guesses, embedding):\n",
        "    # Masked word encoding\n",
        "    char_indices = [26 if c == '_' else ord(c) - ord('a') for c in masked_word.replace(\" \", \"\")]\n",
        "    char_indices_tensor = torch.tensor(char_indices, dtype=torch.long).to(device)\n",
        "    masked_word_embedding = embedding(char_indices_tensor).unsqueeze(0)\n",
        "\n",
        "    # Guessed letters encoding\n",
        "    guessed_vec = [1 if chr(i + ord('a')) in guessed_letters else 0 for i in range(26)]\n",
        "    guessed_tensor = torch.tensor(guessed_vec, dtype=torch.float).to(device)\n",
        "\n",
        "    # Remaining incorrect guesses\n",
        "    remaining_guesses_tensor = torch.tensor([remaining_incorrect_guesses / 6.0], dtype=torch.float).to(device)\n",
        "\n",
        "    # Reshape guessed_tensor and remaining_guesses_tensor to add a dummy dimension for concatenation\n",
        "    guessed_tensor = guessed_tensor.view(1, 1, -1) # Reshape to (1, 1, 26)\n",
        "    remaining_guesses_tensor = remaining_guesses_tensor.view(1, 1, -1) # Reshape to (1, 1, 1)\n",
        "\n",
        "    # Combine all encodings\n",
        "    combined_encoding = torch.cat((masked_word_embedding, guessed_tensor.expand(-1, masked_word_embedding.size(1), -1),\n",
        "                                   remaining_guesses_tensor.expand(-1, masked_word_embedding.size(1), -1)), 2)\n",
        "\n",
        "    return combined_encoding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7TFPc1AcYbdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def encode_state(masked_word, embedding):\n",
        "\n",
        "    char_indices = [26 if c == '_' else ord(c) - ord('a') for c in masked_word.replace(\" \", \"\")]\n",
        "    char_indices_tensor = torch.tensor(char_indices, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # Obtain embeddings for the masked word\n",
        "    masked_word_embedding = embedding(char_indices_tensor).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    return masked_word_embedding\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "KYXO3wvOJICL",
        "outputId": "50c932bd-0d73-4b00-a851-9849618795f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef encode_state(masked_word, embedding):\\n\\n    char_indices = [26 if c == \\'_\\' else ord(c) - ord(\\'a\\') for c in masked_word.replace(\" \", \"\")]\\n    char_indices_tensor = torch.tensor(char_indices, dtype=torch.long).to(device)\\n\\n\\n\\n    # Obtain embeddings for the masked word\\n    masked_word_embedding = embedding(char_indices_tensor).unsqueeze(0)  # Add batch dimension\\n\\n    return masked_word_embedding\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, model, epsilon):\n",
        "    \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    if random.random() > epsilon:  # Exploit\n",
        "        with torch.no_grad():\n",
        "\n",
        "\n",
        "            q_values = model(state)\n",
        "\n",
        "\n",
        "\n",
        "            action = q_values.max(1)[1].item()\n",
        "\n",
        "    else:  # Explore\n",
        "\n",
        "        action = random.randrange(26)\n",
        "\n",
        "    return action\n",
        "\n"
      ],
      "metadata": {
        "id": "Sv-2GxgxJIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "\n",
        "# Define a transition tuple\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of transitions\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "nhLEIf2XJIHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(policy_net, target_net, memory, optimizer, batch_size, gamma, desired_state_length):\n",
        "    if len(memory) < batch_size:\n",
        "        return  # Not enough samples to train\n",
        "\n",
        "    # Sample a batch of transitions from memory\n",
        "    transitions = memory.sample(batch_size)\n",
        "\n",
        "    # Filter transitions to only those with the desired state length\n",
        "    filtered_transitions = [trans for trans in transitions if trans.state.shape[1] == desired_state_length]\n",
        "\n",
        "    # If there are not enough transitions of the desired length, return early\n",
        "    if len(filtered_transitions) < batch_size:\n",
        "        return\n",
        "\n",
        "    # Convert filtered batch-array of Transitions to Transition of batch-arrays\n",
        "    batch = Transition(*zip(*filtered_transitions))\n",
        "\n",
        "    # Rest of the training logic remains the same\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool).to(device)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
        "    state_batch = torch.cat(batch.state).to(device)\n",
        "    action_batch = torch.cat(batch.action).to(device)\n",
        "    reward_batch = torch.cat(batch.reward).to(device)\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_state_values = torch.zeros(len(filtered_transitions)).to(device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "R5wD18yrQLle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_target_network(policy_net, target_net):\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "8THXikIyJIMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_curriculum_datasets(all_words):\n",
        "    level_1 = [word for word in all_words if len(word) < 3]\n",
        "    level_2 = [word for word in all_words if 3 <= len(word) <= 6]\n",
        "    level_3 = [word for word in all_words if len(word) > 6]\n",
        "    return level_1, level_2, level_3\n"
      ],
      "metadata": {
        "id": "GdjEDgepJIOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "policy_net = HangmanModel().to(device)\n",
        "target_net = HangmanModel().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "memory = ReplayMemory(30000)  # Adjust size as needed\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "\n",
        "\n",
        "# Curriculum levels based on word length\n",
        "words_file_path = \"/content/drive/MyDrive/words_250000_train.txt\"\n",
        "with open(words_file_path, 'r') as file:\n",
        "    words = [line.strip().lower() for line in file]\n",
        "\n",
        "# Initialize the Hangman environment\n",
        "\n",
        "curriculum_levels = create_curriculum_datasets(words)\n",
        "level_thresholds = [0.6, 0.5, 0.5]\n",
        "\n",
        "env = HangmanEnvironment(curriculum_levels[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "saQ_OHBHJIQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_win_rate(env, model, words, num_episodes=100, use_validation=False):\n",
        "    win_count = 0\n",
        "    for _ in range(num_episodes):\n",
        "        word = random.choice(words)\n",
        "        state_dict = env.reset(use_validation=use_validation, new_game=True, target_word=word)\n",
        "\n",
        "        while not state_dict['done']:\n",
        "            state_encoding = encode_state(\n",
        "                state_dict['masked_word'],\n",
        "                state_dict['guessed_letters'],\n",
        "                env.max_incorrect_guesses - state_dict['incorrect_guesses'],\n",
        "                policy_net.char_embeddings\n",
        "            )\n",
        "            state_encoding = state_encoding.to(device)\n",
        "\n",
        "            action = select_action(state_encoding, model, epsilon=0.01)  # Use a very low epsilon for evaluation\n",
        "            letter = chr(action + ord('a'))\n",
        "            state_dict, _, done = env.guess(letter)\n",
        "\n",
        "        if '_' not in state_dict['masked_word']:  # Check if the word was completely guessed\n",
        "            win_count += 1\n",
        "\n",
        "    return win_count / num_episodes\n"
      ],
      "metadata": {
        "id": "wmmHv-m-YiXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def evaluate_win_rate(env, model, words, num_episodes=100, use_validation=False):\n",
        "    win_count = 0\n",
        "    for _ in range(num_episodes):\n",
        "        word = random.choice(words)\n",
        "        env.reset(use_validation=use_validation, new_game=True, target_word=word)\n",
        "\n",
        "        state = env.get_state()\n",
        "        state_encoding = encode_state(state['masked_word'], policy_net.char_embeddings)\n",
        "        state_encoding = state_encoding.to(device)\n",
        "\n",
        "        while not state['done']:\n",
        "            action = select_action(state_encoding, model, epsilon=0.01)  # Use a very low epsilon for evaluation\n",
        "            letter = chr(action + ord('a'))\n",
        "            state, _, done = env.guess(letter)\n",
        "            state_encoding = encode_state(state['masked_word'], policy_net.char_embeddings)\n",
        "\n",
        "        if '_' not in state['masked_word']:  # Check if the word was completely guessed\n",
        "            win_count += 1\n",
        "\n",
        "    return win_count / num_episodes\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "IH5b_WBnJITE",
        "outputId": "ef56c121-6df6-4598-e5b9-f445957d61ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef evaluate_win_rate(env, model, words, num_episodes=100, use_validation=False):\\n    win_count = 0\\n    for _ in range(num_episodes):\\n        word = random.choice(words)\\n        env.reset(use_validation=use_validation, new_game=True, target_word=word)\\n\\n        state = env.get_state()\\n        state_encoding = encode_state(state['masked_word'], policy_net.char_embeddings)\\n        state_encoding = state_encoding.to(device)\\n\\n        while not state['done']:\\n            action = select_action(state_encoding, model, epsilon=0.01)  # Use a very low epsilon for evaluation\\n            letter = chr(action + ord('a'))\\n            state, _, done = env.guess(letter)\\n            state_encoding = encode_state(state['masked_word'], policy_net.char_embeddings)\\n\\n        if '_' not in state['masked_word']:  # Check if the word was completely guessed\\n            win_count += 1\\n\\n    return win_count / num_episodes\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_episodes = 25000\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 1000\n",
        "batch_size = 128\n",
        "gamma = 0.8\n",
        "TARGET_UPDATE = 10\n",
        "evaluation_interval = 200\n"
      ],
      "metadata": {
        "id": "VPTt9otFKGpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filename=\"/content/drive/MyDrive/agent_hangman_model_test1.pth\"):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "\n",
        "training_losses = []\n",
        "win_rates = []"
      ],
      "metadata": {
        "id": "PbSKIvdiKGre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make sure to import tqdm at the beginning of your script\n",
        "\n",
        "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
        "    epsilon = epsilon_start + (epsilon_end - epsilon_start) * (episode / epsilon_decay)\n",
        "    state = env.reset(new_game=True)  # Start with a new word\n",
        "    done = False\n",
        "    win = False\n",
        "    total_reward = 0\n",
        "    train_num = 0\n",
        "\n",
        "    while not done:\n",
        "        state_encoding = encode_state(state['masked_word'], state['guessed_letters'],\n",
        "                                      state['incorrect_guesses'], policy_net.char_embeddings)\n",
        "        action = select_action(state_encoding, policy_net, epsilon)\n",
        "        next_state, reward, done = env.guess(chr(action + ord('a')))\n",
        "        total_reward += reward\n",
        "        next_state_encoding = None if done else encode_state(next_state['masked_word'], next_state['guessed_letters'],\n",
        "                                                             next_state['incorrect_guesses'], policy_net.char_embeddings)\n",
        "\n",
        "        memory.push(state_encoding, torch.tensor([[action]], dtype=torch.long), next_state_encoding,\n",
        "                    torch.tensor([reward], dtype=torch.float), done)\n",
        "\n",
        "        state = next_state if not done else state\n",
        "\n",
        "        train_loss = train_model(policy_net, target_net, memory, optimizer, batch_size, gamma,state_encoding.shape[1])\n",
        "        #training_losses.append(train_loss.item())\n",
        "        train_num += 1\n",
        "\n",
        "        if done and reward > 0:\n",
        "            win = True  # Agent won the game\n",
        "            #print(\"episode {} trained with {} times\".format(episode, train_num))\n",
        "        elif done:\n",
        "            win = False  # Agent lost the game\n",
        "            env.reset(new_game=False)  # Reset the environment to the current word\n",
        "            done = False  # Continue training on the same word\n",
        "\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        update_target_network(policy_net, target_net)\n",
        "\n",
        "    if episode % evaluation_interval == 0 and episode > 0:\n",
        "        #train_win_rate = evaluate_win_rate(env, policy_net, env.train_words, num_episodes=200)\n",
        "        val_win_rate = evaluate_win_rate(env, policy_net, env.val_words, num_episodes=200, use_validation=True)\n",
        "        win_rates.append(val_win_rate)\n",
        "\n",
        "        print(f\"Episode: {episode}, Validation Win Rate: {val_win_rate:.2f}\")\n",
        "\n",
        "        if val_win_rate >= 0.25:\n",
        "            print(\"Saving model based on validation performance.\")\n",
        "            save_model(policy_net)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "2uFRQcWhKGt8",
        "outputId": "f7580bb0-6322-4011-8e1a-fa1b11597ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   1%|          | 201/25000 [02:58<196:14:06, 28.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 200, Validation Win Rate: 0.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   2%|▏         | 401/25000 [07:16<249:25:52, 36.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 400, Validation Win Rate: 0.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   2%|▏         | 601/25000 [13:58<219:45:27, 32.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 600, Validation Win Rate: 0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   3%|▎         | 801/25000 [21:52<269:11:32, 40.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 800, Validation Win Rate: 0.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   4%|▍         | 966/25000 [43:30<18:02:22,  2.70s/it] \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-102ee4fad29c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_encoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#training_losses.append(train_loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-0ddfbf6ec32c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(policy_net, target_net, memory, optimizer, batch_size, gamma, desired_state_length)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.plot(training_losses)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RNCbqqaBKGwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.plot(win_rates)\n",
        "plt.legend([\"Training\", \"Validation\"])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uszi6j98FkPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "m9_q5SsaJYQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FxcT_ZeKqWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}